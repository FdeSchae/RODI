wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
C:\Users\DeSchaetzen\Anaconda3\envs\env\lib\site-packages\pytorch_lightning\trainer\connectors\accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.
  rank_zero_deprecation(
C:\Users\DeSchaetzen\Anaconda3\envs\env\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
C:\Users\DeSchaetzen\Anaconda3\envs\env\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(



































































































Finding best initial lr: 100%|███████████████████████████████████████████████████████| 100/100 [27:16<00:00, 16.36s/it]
C:\Users\DeSchaetzen\Anaconda3\envs\env\lib\site-packages\pytorch_lightning\callbacks\model_checkpoint.py:616: UserWarning: Checkpoint directory C:\Users\DeSchaetzen\Desktop\RODI\MachineLearningClassifier\logs\rodi\rodi_resnet18_cross-entropy_b128\f0\230407-1226-d48a exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
C:\Users\DeSchaetzen\Anaconda3\envs\env\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
New lr: 0.000630957344480193

Sanity Checking DataLoader 0:  50%|███████████████████████████                           | 1/2 [00:03<00:03,  3.30s/it]
C:\Users\DeSchaetzen\Anaconda3\envs\env\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
























Epoch 0:  83%|████████████████████████████████████████▌        | 24/29 [04:22<00:54, 10.95s/it, loss=0.815, v_num=d48a]



























Epoch 1:  83%|████████████████████████████████████████▌        | 24/29 [04:21<00:54, 10.89s/it, loss=0.524, v_num=d48a]






















